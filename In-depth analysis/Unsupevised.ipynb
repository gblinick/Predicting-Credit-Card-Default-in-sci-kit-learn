{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use clustering to see if we can find any patterns among customers in our data. This step could and probably should have been performed in the EDA phase of this project, but we include it here because we are performing all Machine Learning techniques together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans()\n",
    "\n",
    "model.fit(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: The variances for our features are very different. Therefore, we need to standardize the input features to avoid giving more importance to features with large variances (that is, the LIMIT_BAL, BILL_AMT, and PAY_AMT variables, which all represent dollar amounts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing K: The Elbow Sum-of-Squares Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ks = range(1,21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented out because takes time to run\n",
    "# inertias = []\n",
    "\n",
    "# for k in ks:\n",
    "#     scaler = StandardScaler()\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     pipeline = make_pipeline(scaler, kmeans)\n",
    "#     pipeline.fit(data.values)\n",
    "#     inertias.append(pipeline.named_steps['kmeans'].inertia_)\n",
    "\n",
    "# with open ('list_of_inertias.txt', 'w') as f:\n",
    "#     for inertia in inertias:\n",
    "#         f.write(\"%f\\n\" % inertia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_of_inertias.txt') as f:\n",
    "    inertias = f.readlines()\n",
    "inertias = [x.strip() for x in inertias]\n",
    "inertias = list(map(float, inertias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, inertias, 'o-')\n",
    "plt.xlabel('K Values')\n",
    "plt.ylabel('Inertia Values')\n",
    "plt.title('Inertia by K Value with Scaling Beforehand')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like 3 or 4 clusters is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "declines = dict()\n",
    "\n",
    "for i in range(1, len(inertias)):\n",
    "    declines[\"Difference Between \" + str(i+1) + \" clusters and \"+ str(i) + \" clusters\"] = \"{:,}\".format(inertias[i] - inertias[i-1])\n",
    "declines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms that 3 clusters is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "pipeline.fit(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.named_steps['kmeans'].labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the values fall into group 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(unique, counts, tick_label=unique)\n",
    "plt.xlabel(\"Cluster #\")\n",
    "plt.ylabel(\"Number of Points\")\n",
    "plt.title(\"Number of Points per Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing K: The Silhouette Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Did not run SIlhouette Method because it takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# silhouette_scores = []\n",
    "\n",
    "# range_n_clusters = range(2,21)\n",
    "\n",
    "# start_total = time.time()\n",
    "# for n_clusters in range_n_clusters:\n",
    "#     loop_start = time.time()\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     pipeline = make_pipeline(scaler, kmeans)\n",
    "#     cluster_labels = pipeline.named_steps['kmeans'].fit_predict(data.values)\n",
    "\n",
    "#     # The silhouette_score gives the average value for all the samples.\n",
    "#     # This gives a perspective into the density and separation of the formed\n",
    "#     # clusters\n",
    "#     silhouette_avg = silhouette_score(data.values, cluster_labels)\n",
    "#     silhouette_scores.append(silhouette_avg)\n",
    "#     print(\"For n_clusters =\", n_clusters,\n",
    "#           \"The average silhouette_score is :\", silhouette_avg)\n",
    "#     loop_end = time.time()\n",
    "#     print(\"For n_clusters =\", n_clusters, \"The loop ran for {0} seconds \".format(loop_end - loop_start))\n",
    "    \n",
    "\n",
    "# end_total = time.time()\n",
    "\n",
    "# total = end_total - start_total\n",
    "# print(\"Total time: \", total)\n",
    "\n",
    "\n",
    "# with open ('list_of_silhouettes.txt', 'w') as f:\n",
    "#     for silhouettes in silhouette_scores:\n",
    "#         f.write(\"%f\\n\" % silhouette)\n",
    "\n",
    "# with open('list_of_silhouettes.txt') as f:\n",
    "#     silhouettes = f.readlines()\n",
    "# silhouettes = [x.strip() for x in silhouettes]\n",
    "# silhouettes = list(map(float, silhouettes))\n",
    "\n",
    "\n",
    "# plt.plot(n_clusters, silhouettes, 'o-')\n",
    "# plt.xlabel('n_clusters')\n",
    "# plt.ylabel('Silhouette Scores')\n",
    "# plt.title('Silhouette Scores by n_cluseters')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy  import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergings = linkage(data.values, method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendrogram(mergings,\n",
    "#            labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca.fit(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed = pca.transform(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KMeans(n_clusters=10, random_state=10)\n",
    "# model.fit(x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(transformed[:,0], transformed[:,1], s=50, c=model.labels_)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions About Clustering:\n",
    "\n",
    "1. How should I go about clustering since I have no idea about how many groupings there are in advance?\n",
    "2. Should I group by only a subset of features?\n",
    "3. How to do proper scaling for various clustering techniques?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
